import hashlib
import time
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional
from urllib.parse import parse_qs, urlparse

import requests
from bs4 import BeautifulSoup

from career_matcher.configs import settings
from career_matcher.crawler.models import JobPosting


def extract_job_data(card: BeautifulSoup) -> Dict[str, Any]:
    """í•˜ë‚˜ì˜ ì±„ìš© ê³µê³  ì¹´ë“œì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ê¸°ìˆ (skills) ì¶”ì¶œ ë¡œì§ ì œê±°ë¨."""

    # 1. ì œëª© ë° URL
    title_el = card.select_one('h2.job_tit a')
    title = title_el.get_text(strip=True) if title_el else 'N/A'
    relative_url = title_el.get('href', '') if title_el else ''
    url = 'https://www.saramin.co.kr' + relative_url if relative_url else ''

    # 2. íšŒì‚¬ëª…
    company_el = card.select_one('strong.corp_name a') or card.select_one('strong.corp_name')
    company = company_el.get_text(strip=True) if company_el else 'N/A'

    # 3. ì£¼ìš” ì¡°ê±´ ì¶”ì¶œ (ê·¼ë¬´ì§€, ê²½ë ¥, í•™ë ¥, ê¸‰ì—¬, ë§ˆê°ì¼ ë“±)
    conditions_el = card.select_one('.job_condition')

    # job_condition ë‚´ì˜ spanë“¤ì„ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
    condition_spans = conditions_el.select('span') if conditions_el else []

    # í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    conditions = [span.get_text(strip=True) for span in condition_spans if span.get_text(strip=True)]

    # ì¡°ê±´ë“¤ì„ êµ¬ë¶„ìì— ë”°ë¼ ë¶„ë¦¬í•˜ì—¬ ì €ì¥ (ë‚˜ì¤‘ì— ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ ë¶„ë¦¬ëœ ì±„ë¡œ ìœ ì§€)
    location = conditions[0] if len(conditions) > 0 else 'N/A'
    career_education = conditions[1] if len(conditions) > 1 else 'N/A'
    salary_etc = conditions[2] if len(conditions) > 2 else 'N/A'

    # ê²½ë ¥/í•™ë ¥ ë¶„ë¦¬ ì‹œë„ (ì™„ë²½í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
    career = 'N/A'
    education = 'N/A'
    if 'ì‹ ì…' in career_education or 'ê²½ë ¥' in career_education or 'ë…„' in career_education:
        career = career_education
    elif 'ì¡¸' in career_education or 'ë ¥ë¬´ê´€' in career_education:
        education = career_education

    # 4. ì§ë¬´ ì¹´í…Œê³ ë¦¬
    job_category_els = card.select('.job_sector a')
    job_categories = [a.get_text(strip=True) for a in job_category_els]
    job_category = ", ".join(job_categories)

    # 5. ê¸°ìˆ /í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ì€ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ **ì œê±°ë¨**

    # 6. ë§ˆê°ì¼ (due_date) ì¶”ì¶œ ì‹œë„
    date_els = card.select('.job_date span')
    due_date = 'N/A'
    if date_els:
        date_text = date_els[0].get_text(strip=True)
        if '~' in date_text:  # ë§ˆê°ì¼ ì •ë³´ê°€ "~"ë¡œ ì‹œì‘í•˜ëŠ” í˜•íƒœì¼ ê²½ìš°
            due_date = date_text

    return {
        'title': title,
        'company': company,
        'url': url,
        'location': location,
        'career': career,
        'education': education,
        'salary_etc': salary_etc,
        'job_category': job_category,
        'due_date': due_date  # 'skills' í•„ë“œê°€ ì œê±°ë¨
    }


def crawl_saramin_job_postings(
    search_keyword: str,
    pages: Optional[int] = None,
    delay: float = settings.DEFAULT_LIST_DELAY,
) -> Iterable[JobPosting]:
    """Saraminì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ê³µê³ ë¥¼ í¬ë¡¤ë§í•˜ê³  JobPosting ëª©ë¡ì„ ë°˜í™˜í•œë‹¤."""
    all_job_data: List[JobPosting] = []
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    max_pages = pages or settings.DEFAULT_MAX_PAGES

    print(f"âœ… ê²€ìƒ‰ í‚¤ì›Œë“œ: '{search_keyword}'ë¡œ ìµœëŒ€ {settings.MAX_JOB_COUNT}ê°œì˜ ê³µê³  í¬ë¡¤ë§ ì‹œì‘...")

    # í˜ì´ì§€ ë°˜ë³µ
    for page in range(1, max_pages + 1):
        if len(all_job_data) >= settings.MAX_JOB_COUNT:
            break

        params = {
            'search_area': 'main',
            'search_done': 'y',
            'searchType': 'default_mysearch',
            'searchword': search_keyword,
            'recruitPage': page,
            'recruitSort': 'relation',
            'recruitPageCount': settings.JOBS_PER_PAGE
        }

        try:
            response = requests.get(base_url, params=params, headers=headers)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"ğŸš¨ í˜ì´ì§€ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í˜ì´ì§€ {page}): {e}")
            break

        soup = BeautifulSoup(response.text, 'html.parser')
        job_cards = soup.select('div.item_recruit')

        if not job_cards:
            print(f"â„¹ï¸ í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ê° ê³µê³  ì¹´ë“œ ë°ì´í„° ì¶”ì¶œ
        for card in job_cards:
            if len(all_job_data) >= settings.MAX_JOB_COUNT:
                break

            job_data = extract_job_data(card)
            job_posting = to_job_posting(job_data)
            all_job_data.append(job_posting)

        print(f"âœ”ï¸ í˜ì´ì§€ {page} ì²˜ë¦¬ ì™„ë£Œ. í˜„ì¬ ê³µê³  ìˆ˜: {len(all_job_data)}ê°œ")
        time.sleep(delay)  # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í˜ì´ì§€ë‹¹ ì§€ì—°

    return all_job_data


def extract_job_id(url: str) -> str:
    """URLì˜ rec_idx(ë˜ëŠ” idx) íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ job_idë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    parsed = urlparse(url)
    query = parse_qs(parsed.query)
    for key in ("rec_idx", "idx"):
        if query.get(key):
            return query[key][0]
    path_digits = "".join(filter(str.isdigit, parsed.path))
    if path_digits:
        return path_digits
    seed = f"{url}-{datetime.utcnow().timestamp()}"
    return hashlib.md5(seed.encode("utf-8")).hexdigest()


def to_job_posting(job_data: Dict[str, Any]) -> JobPosting:
    """ìŠ¤í¬ë© ê²°ê³¼ dictë¥¼ JobPosting dataclassë¡œ ë³€í™˜."""
    url = job_data.get("url", "")
    job_id = extract_job_id(url) if url else hashlib.md5(job_data["title"].encode("utf-8")).hexdigest()
    return JobPosting(
        job_id=job_id,
        title=job_data.get("title", "N/A"),
        company=job_data.get("company", "N/A"),
        location=job_data.get("location", "N/A"),
        salary=job_data.get("salary_etc", "N/A"),
        job_category=job_data.get("job_category", ""),
        career=job_data.get("career", "N/A"),
        education=job_data.get("education", "N/A"),
        due_date=job_data.get("due_date", "N/A"),
        url=url,
        skills="",
        posted_at=None,
        closes_at=job_data.get("due_date"),
        summary="",
        scraped_at=datetime.utcnow(),
    )
