# 커리어-매칭 RAG 프로젝트 설명서

이 문서는 `career_matcher` 폴더 내 프로토타입을 빠르게 이해하고 실행하기 위한 운영 가이드입니다.

---

## 1. 구성요소 개요
| 파일 | 역할 |
| --- | --- |
| `career_matcher/crawler/crawler.py` | 사람인 목록 페이지를 키워드 기반으로 수집 (저장은 storage에서 처리) |
| `career_matcher/crawler/storage.py` | 수집 결과를 SQLite/CSV로 저장 |
| `career_matcher/processing/keyword_parser.py` | 사용자가 입력한 직무/스킬/자기소개 문장에서 검색 키워드·경력·지역 정보를 추출 |
| `README.md` | 모듈 요약 및 명령 예시 |
| `기획안.md` | 전체 서비스 기획(데이터 흐름, RAG 구조, 일정) |
| `career_matcher/app/cli.py` | 프로필 입력을 받아 crawler까지 실행하는 헬퍼 |
| `career_matcher/embedding/vector_pipeline.py` | jobs.db → Chroma 벡터 DB 생성 |
| `career_matcher/retriever/rag_retriever.py` | dragonkue 임베딩 + BGE reranker retriever |
| `설명서.md` | 본 문서. 실행 순서와 통합 플로우 정리 |

---

## 2. 준비 사항
1. Python 3.8+ 환경
2. 필수 패키지
   ```bash
   pip install --user requests beautifulsoup4
   ```
3. 네트워크 정책 준수: 사람인 로봇 배제/이용약관을 확인하고 비상업적·연구 목적 범위뿐 아니라 요청 빈도(기본 2초 이상)도 지켜야 합니다.

---

## 3. 실행 순서
### STEP 1) 사용자 입력 정규화
자유형 입력을 파서에 통과시켜 직무/스킬 키워드를 얻습니다.
```bash
python main.py profile "3년차 백엔드인데 LLM 쪽 데이터 분석가로 전환하고 싶어요" --json
```
- 출력 필드
  - `job_terms`, `skill_terms`, `location_terms`
  - `experience_years` 혹은 `seniority_label`
  - `crawler_keywords`, `rag_query_terms` : 이후 크롤링 및 RAG 검색에 바로 사용

### STEP 2) 키워드별 채용공고 수집
파서 결과의 `crawler_keywords`나 직접 지정한 키워드를 크롤러에 전달합니다.
```bash
python main.py crawl --profile "데이터 분석가" --pages 3 --delay 2 --export-csv
```
- 결과
  - SQLite `career_matcher/data/jobs.db`에 `job_postings` 테이블 upsert
  - CSV 백업 `career_matcher/data/csv/키워드_타임스탬프.csv`

### STEP 3) 데이터 점검
```bash
sqlite3 career_matcher/data/jobs.db 'SELECT COUNT(*) FROM job_postings;'
sqlite3 career_matcher/data/jobs.db 'SELECT job_id,title,company FROM job_postings LIMIT 5;'
```
- 이후 단계: 전처리 → 임베딩 → 벡터DB 적재 → RAG 체인 연결(기획안 참조)

### STEP 4) 벡터 DB & Reranker 준비
```bash
python main.py embed --limit 1000
python -m career_matcher.retriever.rag_retriever  # 또는 RerankedJobRetriever import
```

---

## 4. 추천 통합 플로우
1. 사용자 입력 수집 → `keyword_parser.py`로 정규화
2. 필요한 키워드 리스트를 스케줄러에 전달 → `main.py crawl` 실행
3. DB/CSV 적재 후, 별도 파이프라인에서 임베딩 및 벡터 DB에 upsert
4. Streamlit RAG UI에서
   - `profile_builder` 출력으로 세션 프로필 업데이트
   - `rag_query_terms` 기반으로 벡터 검색 → LLM 응답 생성
   - 멀티턴에서 추가 입력 발생 시 다시 파서 호출해 키워드 보강

---

## 5. 주의사항 및 베스트 프랙티스
- **법적 준수**: 크롤링 전에 대상 사이트의 정책을 확인하고, 과도한 병렬/짧은 간격 호출은 지양합니다.
- **카탈로그 확장**: `keyword_parser.py`에 정의된 직무/스킬/지역 카탈로그를 팀 도메인에 맞게 지속적으로 업데이트하세요.
- **국문/영문 혼용**: 파서가 기본 소문자 비교를 하므로 영문 키워드도 그대로 입력 가능합니다. 추가 특수 표기를 쓰는 경우 variants 리스트에 등록합니다.
- **상세 요약 옵션**: `--fetch-summary`는 공고당 추가 요청을 발생시키므로, 네트워크 여건에 따라 `--summary-delay`를 0.5초 이상으로 설정하세요.
- **LLM 연계**: 현재 프로필 파서는 규칙 기반입니다. 추후 LLM을 사용해 입력을 태깅하거나, RAG 질의 전에 요약/정규화를 할 수 있습니다.
- **Reranker 사용**: `rag_retriever.build_reranked_retriever()`를 이용하면 dragonkue 벡터 검색 + `BAAI/bge-reranker-large` 교차 인코더 재순위를 RAG 체인에 그대로 적용할 수 있습니다.
- **데이터 보안**: 수집된 공고 데이터는 내부 분석 용도로만 활용하고 외부 재배포를 피하십시오.

---

## 6. 향후 과제 체크리스트
- [ ] 크롤러 스케줄링 (cron, Airflow 등) 및 키워드 관리
- [ ] `job_profiles` 테이블에 직무 설명/트렌드 문서 적재
- [ ] 임베딩 파이프라인 / 벡터 DB 구축
- [ ] Streamlit RAG 챗봇 UI 및 프롬프트 설계
- [ ] 직무 매칭 점수 로직 고도화 (필드 기반 가중치, reranker 등)

이 설명서를 기준으로 각 단계 산출물을 업데이트하고, 변경 사항은 `기획안.md`와 함께 버전 관리해 주세요.
