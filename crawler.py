import csv
import hashlib
import os
import sqlite3
import time
from datetime import datetime
from typing import Any, Dict, List
from urllib.parse import parse_qs, urlparse

import requests
from bs4 import BeautifulSoup

# ìµœëŒ€ ê°€ì ¸ì˜¬ ê³µê³  ìˆ˜ ì„¤ì • (40ê°œì”© í˜ì´ì§€ë¥¼ ê³„ì‚°í•˜ì—¬ 300ê°œì— ë§ì¶¤)
MAX_JOB_COUNT = 300
JOBS_PER_PAGE = 40
MAX_PAGES = (MAX_JOB_COUNT + JOBS_PER_PAGE - 1) // JOBS_PER_PAGE  # 8 í˜ì´ì§€
DB_PATH = os.path.join("career_matcher", "jobs.db")


def extract_job_data(card: BeautifulSoup) -> Dict[str, Any]:
    """í•˜ë‚˜ì˜ ì±„ìš© ê³µê³  ì¹´ë“œì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ê¸°ìˆ (skills) ì¶”ì¶œ ë¡œì§ ì œê±°ë¨."""

    # 1. ì œëª© ë° URL
    title_el = card.select_one('h2.job_tit a')
    title = title_el.get_text(strip=True) if title_el else 'N/A'
    relative_url = title_el.get('href', '') if title_el else ''
    url = 'https://www.saramin.co.kr' + relative_url if relative_url else ''

    # 2. íšŒì‚¬ëª…
    company_el = card.select_one('strong.corp_name a') or card.select_one('strong.corp_name')
    company = company_el.get_text(strip=True) if company_el else 'N/A'

    # 3. ì£¼ìš” ì¡°ê±´ ì¶”ì¶œ (ê·¼ë¬´ì§€, ê²½ë ¥, í•™ë ¥, ê¸‰ì—¬, ë§ˆê°ì¼ ë“±)
    conditions_el = card.select_one('.job_condition')

    # job_condition ë‚´ì˜ spanë“¤ì„ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
    condition_spans = conditions_el.select('span') if conditions_el else []

    # í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    conditions = [span.get_text(strip=True) for span in condition_spans if span.get_text(strip=True)]

    # ì¡°ê±´ë“¤ì„ êµ¬ë¶„ìì— ë”°ë¼ ë¶„ë¦¬í•˜ì—¬ ì €ì¥ (ë‚˜ì¤‘ì— ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ ë¶„ë¦¬ëœ ì±„ë¡œ ìœ ì§€)
    location = conditions[0] if len(conditions) > 0 else 'N/A'
    career_education = conditions[1] if len(conditions) > 1 else 'N/A'
    salary_etc = conditions[2] if len(conditions) > 2 else 'N/A'

    # ê²½ë ¥/í•™ë ¥ ë¶„ë¦¬ ì‹œë„ (ì™„ë²½í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
    career = 'N/A'
    education = 'N/A'
    if 'ì‹ ì…' in career_education or 'ê²½ë ¥' in career_education or 'ë…„' in career_education:
        career = career_education
    elif 'ì¡¸' in career_education or 'ë ¥ë¬´ê´€' in career_education:
        education = career_education

    # 4. ì§ë¬´ ì¹´í…Œê³ ë¦¬
    job_category_els = card.select('.job_sector a')
    job_categories = [a.get_text(strip=True) for a in job_category_els]
    job_category = ", ".join(job_categories)

    # 5. ê¸°ìˆ /í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ì€ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ **ì œê±°ë¨**

    # 6. ë§ˆê°ì¼ (due_date) ì¶”ì¶œ ì‹œë„
    date_els = card.select('.job_date span')
    due_date = 'N/A'
    if date_els:
        date_text = date_els[0].get_text(strip=True)
        if '~' in date_text:  # ë§ˆê°ì¼ ì •ë³´ê°€ "~"ë¡œ ì‹œì‘í•˜ëŠ” í˜•íƒœì¼ ê²½ìš°
            due_date = date_text

    return {
        'title': title,
        'company': company,
        'url': url,
        'location': location,
        'career': career,
        'education': education,
        'salary_etc': salary_etc,
        'job_category': job_category,
        'due_date': due_date  # 'skills' í•„ë“œê°€ ì œê±°ë¨
    }


def crawl_saramin_job_postings(search_keyword: str = "ë°ì´í„° ë¶„ì„") -> List[Dict[str, Any]]:
    """ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ì±„ìš© ê³µê³ ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    all_job_data = []
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    print(f"âœ… ê²€ìƒ‰ í‚¤ì›Œë“œ: '{search_keyword}'ë¡œ ìµœëŒ€ {MAX_JOB_COUNT}ê°œì˜ ê³µê³  í¬ë¡¤ë§ ì‹œì‘...")

    # í˜ì´ì§€ ë°˜ë³µ
    for page in range(1, MAX_PAGES + 1):
        if len(all_job_data) >= MAX_JOB_COUNT:
            break

        params = {
            'search_area': 'main',
            'search_done': 'y',
            'searchType': 'default_mysearch',
            'searchword': search_keyword,
            'recruitPage': page,
            'recruitSort': 'relation',
            'recruitPageCount': JOBS_PER_PAGE
        }

        try:
            response = requests.get(base_url, params=params, headers=headers)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"ğŸš¨ í˜ì´ì§€ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í˜ì´ì§€ {page}): {e}")
            break

        soup = BeautifulSoup(response.text, 'html.parser')
        job_cards = soup.select('div.item_recruit')

        if not job_cards:
            print(f"â„¹ï¸ í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ê° ê³µê³  ì¹´ë“œ ë°ì´í„° ì¶”ì¶œ
        for card in job_cards:
            if len(all_job_data) >= MAX_JOB_COUNT:
                break

            job_data = extract_job_data(card)
            all_job_data.append(job_data)

        print(f"âœ”ï¸ í˜ì´ì§€ {page} ì²˜ë¦¬ ì™„ë£Œ. í˜„ì¬ ê³µê³  ìˆ˜: {len(all_job_data)}ê°œ")
        time.sleep(1)  # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í˜ì´ì§€ë‹¹ 1ì´ˆ ì§€ì—°

    return all_job_data


def save_to_csv(data: List[Dict[str, Any]], filename: str):
    """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not data:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # CSV í—¤ë” (ì»¬ëŸ¼ëª…). 'skills' í•„ë“œê°€ ì œê±°ë¨
    fieldnames = list(data[0].keys())

    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(data)}ê°œì˜ ê³µê³ ë¥¼ '{filename}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def ensure_job_table(conn: sqlite3.Connection) -> None:
    """job_postings í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ê³ , í•„ìš”í•œ ì»¬ëŸ¼ì„ ë³´ì¥í•©ë‹ˆë‹¤."""
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS job_postings (
            job_id TEXT PRIMARY KEY,
            title TEXT,
            company TEXT,
            location TEXT,
            salary TEXT,
            skills TEXT,
            posted_at TEXT,
            closes_at TEXT,
            url TEXT,
            scraped_at TEXT,
            career TEXT,
            education TEXT,
            job_category TEXT,
            due_date TEXT,
            summary TEXT
        );
        """
    )
    conn.commit()


def extract_job_id(url: str) -> str:
    """URLì˜ rec_idx(ë˜ëŠ” idx) íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ job_idë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    parsed = urlparse(url)
    query = parse_qs(parsed.query)
    for key in ("rec_idx", "idx"):
        if query.get(key):
            return query[key][0]
    path_digits = "".join(filter(str.isdigit, parsed.path))
    if path_digits:
        return path_digits
    seed = f"{url}-{datetime.utcnow().timestamp()}"
    return hashlib.md5(seed.encode("utf-8")).hexdigest()


def upsert_jobs_to_db(data: List[Dict[str, Any]], db_path: str = DB_PATH):
    """í¬ë¡¤ë§ ë°ì´í„°ë¥¼ SQLite DB(job_postings)ì— upsertí•©ë‹ˆë‹¤."""
    if not data:
        print("DBì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    os.makedirs(os.path.dirname(db_path) or ".", exist_ok=True)
    conn = sqlite3.connect(db_path)
    ensure_job_table(conn)

    scraped_at = datetime.utcnow().isoformat()
    rows = []
    for job in data:
        url = job.get("url", "")
        job_id = extract_job_id(url) if url else hashlib.md5(job["title"].encode("utf-8")).hexdigest()
        rows.append(
            (
                job_id,
                job.get("title", "N/A"),
                job.get("company", "N/A"),
                job.get("location", "N/A"),
                job.get("salary_etc", "N/A"),
                "",
                None,
                job.get("due_date"),
                url,
                scraped_at,
                job.get("career", "N/A"),
                job.get("education", "N/A"),
                job.get("job_category", ""),
                job.get("due_date"),
                "",
            )
        )

    conn.executemany(
        """
        INSERT INTO job_postings (
            job_id, title, company, location, salary, skills,
            posted_at, closes_at, url, scraped_at,
            career, education, job_category, due_date, summary
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ON CONFLICT(job_id) DO UPDATE SET
            title=excluded.title,
            company=excluded.company,
            location=excluded.location,
            salary=excluded.salary,
            skills=excluded.skills,
            posted_at=excluded.posted_at,
            closes_at=excluded.closes_at,
            url=excluded.url,
            scraped_at=excluded.scraped_at,
            career=excluded.career,
            education=excluded.education,
            job_category=excluded.job_category,
            due_date=excluded.due_date,
            summary=excluded.summary;
        """,
        rows,
    )
    conn.commit()
    conn.close()
    print(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ! {len(rows)}ê°œì˜ ê³µê³ ë¥¼ '{db_path}'ì— ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.")


# --- ë©”ì¸ ì‹¤í–‰ ---
search_keyword = "ë°ì´í„° ë¶„ì„"  # ì›í•˜ì‹œëŠ” í‚¤ì›Œë“œë¡œ ë³€ê²½ ê°€ëŠ¥
csv_filename = 'saramin_job_postings_no_skills.csv'  # íŒŒì¼ëª…ì„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
db_path = DB_PATH  # í•„ìš” ì‹œ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.

# 1. í¬ë¡¤ë§ ì‹¤í–‰
crawled_jobs = crawl_saramin_job_postings(search_keyword)

# 2. CSV íŒŒì¼ ì €ì¥
save_to_csv(crawled_jobs, csv_filename)

# 3. SQLite DB ì €ì¥
upsert_jobs_to_db(crawled_jobs, db_path)
